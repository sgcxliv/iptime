
- Users must be able to initiate a processing job for a URL of their choosing, and multiple URLs must be able to be processed simultaneously.
- Users must be able to view information on the status of ongoing or completed jobs. For ongoing jobs, you must use an approach where the server pushes updates to the client -- polling will be penalized. The more granular the status updates the better.
- The following rules will define the requirements for the processing pipeline. These requirements are different depending on the type of document located at the given URL
  - HTML document: Get the document, extract the text nodes, optionally clean the text, chunk and embed (fake) the text. On the frontend, the user must be able to see the full html document, the extracted text, and then each chunk and its corresponding embedding vector.
  - PDF document: Get the document, convert to images, OCR the images (fake), chunk (handle the text from each page separately) and embed (fake) the text. On the frontend, the user must be able to see the page images, and for each page should be able to see the extracted text, and then each chunk and its corresponding embedding vector.
- Document Groups
  - Add named document groups, such that any given document can belong to multiple groups (for example a company group and a product line group).
  - Add group relationships, such that groups can have parents and children. (e.g. a document that belongs to a child group should also belong to the parent group)
  - Allow the user to easily curate the group(s) that documents belong to
  - Allow the user to search, sort, and filter the documents by type, group(s), etc
- Add hydration to HTML documents -- many HTML documents are blank until hydrated by javascript code (for example in React websites).
- Extract additional metadata from HTML documents, such as the page title, and use it in the UI
- Display thumbnails for HTML documents
- Display thumbnails for PDF documents
- Add a PDF viewer to the frontend.
- Implement a crawling feature, such that instead of providing a list of urls the user can provide a single starting url and a set of inclusion/exclusion regexes, and the crawler will crawl the web starting from the given url, applying the inclusion/exclusion regexes to determine whether to crawl a given url.
- Add https support (on the remote VM) so that I can type `https://{your_vm_ip}:3001/` into my browser and securely access your app.
- For direction regarding UI design, the `ui-inspiration` directory contains some screenshots 
- For file storage (e.g. pdf documents, images) you can either use the VM's file system or the mongodb database (just remember that mongodb has a document size limit of 16MB, so you will need to use GridFS)
- Some technologies that we use as part of our production stack are as follows:
  - Frontend: typescript, react, zustand, react/tanstack query, tanstack router, tailwindcss, vite
  - Backend: nodejs, expressjs, mongodb
